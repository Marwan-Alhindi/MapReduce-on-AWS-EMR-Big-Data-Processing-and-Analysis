{\rtf1\ansi\ansicpg1252\cocoartf2708
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\froman\fcharset0 TimesNewRomanPSMT;\f2\fmodern\fcharset0 CourierNewPSMT;
\f3\fnil\fcharset0 Menlo-Regular;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red11\green76\blue180;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;\cssrgb\c1961\c38824\c75686;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs32 \cf2 \expnd0\expndtw0\kerning0
This repository contains the codes for running three different MapReduce tasks on AWS EMR. Below are the instructions for deploying and running these tasks.\
\'a0\
# Prerequisites\
\'a0\
- AWS CLI installed and configured\
- Hadoop 3.x\
- Python 3.x\
\'a0\
# Directory Structure\
\'a0\
- `/input/` - This directory contains the input files `Trips.txt` and `Taxis.txt`.\
- `/output/` - This directory will contain the output of the MapReduce jobs.\
\'a0 - `/output/task1`\
\'a0 - `/output/task2`\
\'a0 - `/output/task3`\
- `mapper_t?.py` - The Mapper code where ? is 1 or 2 or 3 indicating task number.\
- `reducer_t?.py` - The Reducer code.\
- `run_t?.sh` - Shell script to run the MapReduce jobs.\
- hadoop-streaming-3.1.4.jar - Hadoop streaming file that allows python scripts to be ran.\
\'a0\
# Steps to Run the Codes on AWS EMR\
\'a0\
1- Open terminal.\
2- Make sure you are in home directory by running the following command:\
\pard\pardeftab720\li960\fi-480\partightenfactor0
\cf2 -
\f1\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0\'a0 
\f0\fs32 cd ~\
\pard\pardeftab720\partightenfactor0
\cf2 3- Move the files from local machine to jumphost by the following command:\
\pard\pardeftab720\li960\fi-480\partightenfactor0
\cf2 -
\f1\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0\'a0 
\f0\fs32 scp -i "path/to/ssh-key.pem" "path/to/local/files/*" username@remote-server-address:/path/to/remote/directory/\
-
\f1\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0\'a0 
\f0\fs32 For example, mine would be like this:\
\pard\pardeftab720\li1920\fi-480\partightenfactor0

\f2 \cf2 o
\f1\fs18\fsmilli9333 \'a0\'a0 
\f0\fs32 scp -i "/Users/marwan/Desktop/Big Data/s3969393-cosc2637.pem" /Users/marwan/Desktop/s3969393_BDP_A1/* {\field{\*\fldinst{HYPERLINK "mailto:ec2-user@s3969393.jump.cosc2637.route53.aws.rmit.edu.au:/home/ec2-user/"}}{\fldrslt \cf3 \ul \ulc3 ec2-user@s3969393.jump.cosc2637.route53.aws.rmit.edu.au:/home/ec2-user/}}\
\pard\pardeftab720\partightenfactor0
\cf2 4- Enter jumphost by running the following command:\
\pard\pardeftab720\li960\fi-480\partightenfactor0
\cf2 -
\f1\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0\'a0 
\f0\fs32 ssh jumphost\
\pard\pardeftab720\partightenfactor0
\cf2 5- Create a cluster in master node by running the following command:\
\pard\pardeftab720\li960\fi-480\partightenfactor0
\cf2 -
\f1\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0\'a0 
\f0\fs32 ./create_cluster.sh\
\pard\pardeftab720\partightenfactor0
\cf2 6- Move all files from jumphost to master node by running the following command:\
\pard\pardeftab720\li960\fi-480\partightenfactor0
\cf2 -
\f1\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0\'a0 
\f0\fs32 scp -i "path/to/ssh-key.pem" local-files username@remote-server-address:/path/to/remote/directory/\
-
\f1\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0\'a0 
\f3\fs29\fsmilli14667 For example, mine would be like this:
\f0\fs32 \
\pard\pardeftab720\li1920\fi-480\partightenfactor0

\f2 \cf2 o
\f1\fs18\fsmilli9333 \'a0\'a0 
\f3\fs29\fsmilli14667 scp -i "/home/ec2-user/s3969393-cosc2637.pem" * {\field{\*\fldinst{HYPERLINK "mailto:hadoop@s3969393.emr.cosc2637.route53.aws.rmit.edu.au:/home/hadoop/"}}{\fldrslt \cf3 \ul \ulc3 hadoop@s3969393.emr.cosc2637.route53.aws.rmit.edu.au:/home/hadoop/}}
\f0\fs32 \
\pard\pardeftab720\partightenfactor0
\cf2 7- Type the following in jumphost to get master node command:\
\pard\pardeftab720\li960\fi-480\partightenfactor0
\cf2 -
\f1\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0\'a0 
\f0\fs32 cat instructions\
-
\f1\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0\'a0 
\f0\fs32 For example, mine would be like this:\
\pard\pardeftab720\li1920\fi-480\partightenfactor0

\f2 \cf2 o
\f1\fs18\fsmilli9333 \'a0\'a0 
\f3\fs29\fsmilli14667 ssh hadoop@s3969393.emr.cosc2637.route53.aws.rmit.edu.au -i s3969393-cosc2637.pem
\f0\fs32 \
\pard\pardeftab720\partightenfactor0
\cf2 8- Give permission to files by running the following command:\
\pard\pardeftab720\li960\fi-480\partightenfactor0
\cf2 -
\f1\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0\'a0 
\f0\fs32 chmod 700 *\
\pard\pardeftab720\partightenfactor0
\cf2 9- Create directories:\
\pard\pardeftab720\li960\fi-480\partightenfactor0
\cf2 -
\f1\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0\'a0 
\f0\fs32 hdfs dfs -mkdir /input\
-
\f1\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0\'a0 
\f0\fs32 hdfs dfs -mkdir /output\
-
\f1\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0\'a0 
\f0\fs32 hdfs dfs -mkdir /output/task1\
-
\f1\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0\'a0 
\f0\fs32 hdfs dfs -mkdir /output/task2\
-
\f1\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0\'a0 
\f0\fs32 hdfs dfs -mkdir /output/task3\
\pard\pardeftab720\partightenfactor0
\cf2 10- Move Trips.txt and Taxis.txt to input directory:\
\pard\pardeftab720\li960\fi-480\partightenfactor0
\cf2 -
\f1\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0\'a0 
\f0\fs32 hdfs dfs -copyFromLocal Trips.txt /input/\
-
\f1\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0\'a0 
\f0\fs32 hdfs dfs -copyFromLocal Taxis.txt /input/\
\pard\pardeftab720\partightenfactor0
\cf2 11- Run the following commands:\
\pard\pardeftab720\li960\fi-480\partightenfactor0
\cf2 -
\f1\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0\'a0 
\fs32 sudo yum install dos2unix
\f0 \
-
\f1\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0\'a0 
\f3\fs29\fsmilli14667 dos2unix mapper_t1.py mapper_t2.py mapper_t3.py
\f0\fs32 \
\pard\pardeftab720\li960\fi-480\partightenfactor0

\fs29\fsmilli14667 \cf2 -
\f1\fs18\fsmilli9333 \'a0\'a0\'a0 
\f3\fs29\fsmilli14667 dos2unix reducer_t1.py reducer_t2.py reducer_t3.py reader_t2.py
\f0\fs32 \
\pard\pardeftab720\partightenfactor0
\cf2 11- Run tasks:\
\pard\pardeftab720\li960\fi-480\partightenfactor0
\cf2 -
\f1\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0\'a0 
\f0\fs32 ./run_t?.sh where ? either 1,2 or 3\
\pard\pardeftab720\partightenfactor0
\cf2 12- After running task1, merge the output and access it:\
\pard\pardeftab720\li960\fi-480\partightenfactor0
\cf2 -
\f1\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0\'a0 
\f3\fs29\fsmilli14667 hadoop fs -cat /output/task1/part-* | hadoop fs -put - /output/task1/merged_output.txt
\f0\fs32 \
-
\f1\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0\'a0 
\f0\fs32 hdfs dfs -cat /output/task1/merged_output.txt\
\pard\pardeftab720\partightenfactor0
\cf2 13- After running task2, access the output by the following:\
\pard\pardeftab720\li960\fi-480\partightenfactor0
\cf2 -
\f1\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0\'a0 
\f0\fs32 cat centroids1.txt or access from /output/task2\
\pard\pardeftab720\partightenfactor0
\cf2 14- After running task3, merge the output and access it:\
\pard\pardeftab720\li960\fi-480\partightenfactor0
\cf2 -
\f1\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0\'a0 
\f0\fs32 Merge and access join output:\
\pard\pardeftab720\li1920\fi-480\partightenfactor0

\f2 \cf2 o
\f1\fs18\fsmilli9333 \'a0\'a0 
\f3\fs29\fsmilli14667 hadoop fs -cat /output/task3/join/part-* | hadoop fs -put - /output/task3/join/merged_output_join.txt
\f0\fs32 \

\f2 o
\f1\fs18\fsmilli9333 \'a0\'a0 
\f0\fs32 hadoop fs -cat /output/task3/join/merged_output.txt\
\pard\pardeftab720\li960\fi-480\partightenfactor0
\cf2 -
\f1\fs18\fsmilli9333 \'a0\'a0\'a0\'a0\'a0\'a0 
\f0\fs32 Merge and access count output:\
\pard\pardeftab720\li1920\fi-480\partightenfactor0

\f2 \cf2 o
\f1\fs18\fsmilli9333 \'a0\'a0 
\f3\fs29\fsmilli14667 hadoop fs -cat /output/task3/count/part-* | hadoop fs -put - /output/task3/count/merged_output_count.txt
\f0\fs32 \

\f2 o
\f1\fs18\fsmilli9333 \'a0\'a0 
\f0\fs32 hadoop fs -cat /output/task3/count/merged_output_count.txt\
}